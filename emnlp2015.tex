\documentclass[11pt,a4paper]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usetikzlibrary{bayesnet}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{ {images/} }
\setlength\titlebox{5cm}

% Better references, I think
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}

\newcommand{\relation}[1]{\textsc{#1}}
\newcommand{\pathtype}{\ensuremath{\pi}}

\title{Incorporating Knowledge Graph Features\\
into Relation Extraction Models}

%\author{Dheeru Dua \\
  %Carnegie Mellon University \\
  %{\tt ddua@andrew.cmu.edu} \\\And
  %Matt Gardner \\
  %Carnegie Mellon University \\
  %{\tt mg1@cs.cmu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

  There has been much recent work on the construction of large knowledge bases,
  such as Freebase, NELL, and DBPedia.  These knowledge bases (KBs) have been
  used to provide training data for relation extraction systems and semantic
  parsers, and as databases backing question answering systems, but there has
  been very little work that makes use of these KBs for lower-level natural
  language processing tasks.  We present a simple technique for incorporating
  features from a KB into any graphical model that contains factors involving
  KB entities.  We apply this idea to a relation extraction model, showing
  striking improvements over prior relation extraction techniques, both those
  that use distant supervision and those that use KB embeddings.

\end{abstract}

\section{Introduction}

It has long been understood that a large collection of common-sense knowledge
is an essential component of any system that will be able to fully understand
natural language.  The Cyc project~\cite{cyc-1995} is one of the more
well-known projects seeking to collect such common-sense knowledge, and the
recent Winograd schema challenge~\cite{winograd-schema-2012} makes it very
clear how necessary this kind of knowledge is in natural language
understanding.  Because of this necessity, there has recently been a surge of
interest in constructing very large knowledge bases (KBs), such as
Freebase~\cite{freebase-2008-bollacker}, NELL~\cite{nell-2015-aaai}, and
DBPedia~\cite{dbpedia-2012-mendes}, which contain facts about people, things,
and places in the world.  These knowledge bases have found use in providing
training data for relation extraction and semantic parsing
models~\cite{riedel-2010-distant-supervision,krishnamurthy-2012-semantic-parsing},
and forming the foundation of question answering systems such as Google's
search and Apple's Siri.

One place where these knowledge bases have not typically found use is in
lower-level natural language models.  While it is now fairly well understood
how to parse a question into an executable query over a structured knowledge
base, it is less clear how to tractably incorporate a knowledge base into a
model for parsing or part-of-speech tagging.  In this work, we take a step in
that direction by presenting a general technique for incorporating features
generated from a knowledge base into \emph{any} graphical model that contains
factors involving KB entities.

Our method is inspired by the Path Ranking Algorithm~\cite[PRA]{lao-2010-pra},
a method for constructing feature matrices over node pairs in a graph.  Almost
all prior work with PRA has used the constructed feature matrix to perform link
prediction; when the graph corresponds to a knowledge base, this is the task of
knowledge base completion~\cite{lao-2011-pra2,lao-2012-syntactic-pra,%
dong-2014-knowledge-vault,gardner-2013-latent-pra,gardner-2014-vector-space-pra}.
However, the feature matrices constructed by PRA need not be used only in a
classifier for link prediction; they can be included as factors in arbitrary
graphical models.

As an initial proof-of-concept of this idea, we add a PRA feature matrix as a
factor in an off-the-shelf distantly-supervised relation extraction model, the
MultiR system of Hoffmann et al.~\shortcite{hoffmann-2011-distant-supervision}.
We chose this model for our initial experiment because the choice of KB is
obvious (as they use a KB to obtain distantly-supervised training examples),
and deciding how to incorporate a PRA factor is not difficult.  In contrast,
finding a suitable KB for a parsing model, as well as adding a factor in a way
that still allows for efficient inference, are much more challenging (e.g.,
something like ConceptNet~\cite{conceptnet-2004} could be used, though its
coverage may be questionable).  However, the gains we achieve by adding PRA
features into the MultiR model are quite substantial, and we believe they
provide sufficient motivation to explore incorporating these same ideas into
more low-level NLP models.

In the remainder of this paper, we first briefly describe the PRA and MultiR
models, as well as other related work, then we describe how we incorporate PRA
features into MultiR.  We then present an experimental comparison between our
PRA-augmented MultiR model with various techniques from prior work, showing
that our method substantially outperforms prior techniques in both aggregate
and sentential relation extraction.

\section{Related Work}

\subsection{The Path Ranking Algorithm}

The path ranking algorithm was introduced by Lao and
Cohen~\shortcite{lao-2010-pra}.  It is a two-step process for generating a
feature matrix over node pairs in a graph with labeled edges.  The first step
finds a set of potentially useful path types that connect the node pairs, which
become the columns of the feature matrix.  A \emph{path type} \pathtype{} is a
sequence of edge types in the graph, such as
-\relation{CityInState}-\relation{StateInCountry}-.  These path types are found
by performing a search over the graph, looking for path types that commonly
connect the source and target nodes of the training instances for a particular
relation.  Once selected, each path type becomes a column in the constructed
feature matrix (where rows correspond to entity pairs).

Once a set of path types has been found, the second step of PRA then computes
the values in the feature matrix.  The value of the cell at row (entity pair)
$i$ and column (path type) $j$ is the probability that a random walk beginning
at the first entity in pair $i$, and following a path of type $j$, will end at
the second entity in pair $i$: $p(i_2|i_1,j)$.  These probabilities can be
computed with a breadth-first search, with matrix multiplications, or by using
random walks to perform rejection sampling.

The PRA feature matrix is typically used for KB completion by training a
logistic regression classifier to score new entity pairs.  In this work,
however, we inject the feature matrix into a more complex graphical model, the
MultiR model for performing relation extraction, described in the next section.

\subsection{MultiR}

The MultiR algorithm~\cite{hoffmann-2011-distant-supervision} is a
probabilistic graphical model for performing relation extraction with distant
supervision.  The model, shown in \figref{multir}, is \emph{distantly
supervised} because, while it makes predictions on individual sentences, it has
no annotated sentences as training data.  The input to MultiR is a KB and a set
of sentences that each mention a pair of KB entities.  The model assumes that
at least one of the sentences expresses each relationship seen between the
entity pair in the KB, and learns a model to predict which sentences express
which relations.  In the model description in \figref{multir}, $S$ is the set
of sentences between any two entities $e_1$, $e_2$ $\in$ E and $Z_i$'s are the
latent variables that range over all relation names and a distinct value
\relation{None}.  The model outputs a binary relation vector Y, with indicator
bits for each relation.  It is trained using an online perceptron-style
algorithm.

\subsection{Other Related Work}
\label{sec:related-work}

We briefly mention here a few other pieces of related work, particularly those
that we will compare our work against.  Surdeanu et
al.~\shortcite{surdeanu-2012-relation-extraction} presented MIML-RE, which is
very similar to the MultiR algorithm, but relaxes some of the hard constraints
used by MultiR.  Weston et
al.~\shortcite{weston-2013-connecting-language-and-kb} is the work that is most
similar to ours, in that they also attempted to add information from a
knowledge base to a relation extraction model.  They used an ensemble with
neural embeddings, however, instead of the factor-based approach that we take
in this paper, so their technique cannot be applied to arbitrary graphical
models, as ours can.

Additionally, Nakashole and Mitchell~\shortcite{nakashole-2015-pp-attachment}
show a method for incorporating background knowledge into predicting
prepositional phrase attachment.  Their work provides additional motivation
for this line of research, and we believe that the addition of a PRA-style
feature matrix from a suitable KB such as ConceptNet~\cite{conceptnet-2004}
would further improve their results.

\section{PRA-Augmented MultiR}
\label{sec:pra-augmented-multir}

There are two options for adding a PRA factor into the MultiR model.  The first
(and more natural) place to add the factor is once per entity pair, as an
additional component to the factor connecting the $Y$ and $Z_i$ variables.
This is shown in \figref{per-entity-pair}.  The issue with this placement is
that the factor connecting $Y$ and $Z_i$ is a deterministic OR, and the model
is trained using an at-least-once assumption; this means that if the model can
explain the existence of a relationship between an entity pair with the PRA
factor, it will tend to not learn from the sentences for that entity pair.

An alternate placement is to include the PRA features as an additional
component of every mention of the entity pair, shown in \figref{per-mention}.
One way this can be thought of is as a stronger kind of supervision; the
stronger an entity pair exhibits a particular relationship, as determined by
the PRA features, the more likely the dependency parse features of the MultiR
model are to be up-weighted for that sentence.  When judging aggregate
performance, this also gives the model access to a much richer set of features
than just those expressed in the corpus.

\begin{figure}[t!]
  \centering
  \begin{subfigure}[t]{1.7cm}
    \centering
    \begin{tikzpicture}
       \node[latent]          (y)   {$Y$};
       \node[latent, below=2cm of y]            (zi) {$Z_i$} ;
       \factor[below=1.2cm of y] {y-f} {} {y} {zi} ;
       \factor[right=0.3cm of zi] {zi-f} {$f_{s}$} {zi} {} ;
        \plate {r} {
          (y)
        } {$R$} ; %
       \plate {s} {
          (zi)(zi-f)
        } {$S$} ; %
      \plate {ee} {
          (r)(s)
        } {$E \times E$} ; %
    \end{tikzpicture}
    \caption{MultiR}
    \label{fig:multir}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{2.3cm}
    \centering
    \begin{tikzpicture}
       \node[latent]          (y)   {$Y$};
       \node[latent, below=2cm of y]            (zi) {$Z_i$} ;
       \factor[below=1.2cm of y] {y-f} {} {y} {zi} ;
       \node[latent, right=0.3cm of y-f]   (z-pra) {$Z_{pra}$} ;
       \factoredge {z-pra} {y-f} {} ;
       \factor[right=0.3cm of zi] {zi-f} {$f_{s}$} {zi} {} ;
       \factor[above=of z-pra] {zi-fpra} {$f_{pra}$} {z-pra} {} ;
        \plate {r} {
          (y)
        } {$R$} ; %
       \plate {s} {
          (zi)(zi-f)
        } {$S$} ; %
      \plate {ee} {
          (r)(s)(z-pra)(zi-fpra)
        } {$E \times E$} ; %
    \end{tikzpicture}
    \caption{Per Entity-pair}
    \label{fig:per-entity-pair}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{2.0cm}
    \centering
    \begin{tikzpicture}
       \node[latent]          (y)   {$Y$};
       \node[latent, below=2cm of y]            (zi) {$Z_i$} ;
       \factor[below=1.2cm of y] {y-f} {} {y} {zi} ;
       \factor[right=0.3cm of zi] {zi-f} {$f_{s}$} {zi} {} ;
       \factor[left=0.3cm of zi] {zi-fpra} {$f_{pra}$} {zi} {} ;
        \plate {r} {
          (y)
        } {$R$} ; %
       \plate {s} {
          (zi)(zi-f)(zi-fpra)
        } {$S$} ; %
      \plate {ee} {
          (r)(s)
        } {$E \times E$} ; %
    \end{tikzpicture}
    \caption{Per Mention}
    \label{fig:per-mention}
  \end{subfigure}
  \caption{Plate notation of the MultiR model and our modifications of it.  $R$
  is the number of relations in the KB, $E$ is the number of entities, and each
  $Y$ variable encodes whether an entity pair expresses relation $R$.  There is
  one $Z$ variable per sentence, encoding which relation is expressed by that
  sentence.}
\end{figure}


\section{Experiments}

To evaluate the performance of this model, we use the dataset developed by
Riedel et al.~\shortcite{riedel-2010-distant-supervision}, which aligns
Freebase relations with the New York Times corpus.  The dataset contains a
number of sentence mentions for each entity pair. The entities are extracted
from sentences by Stanford Named Entity Tagger~\cite{finkel-2005-non-local-ie},
and they participate in 52 separate relations from Freebase.  The sentence
level features include the part of speech tags, named entities and dependency
tree paths.  The evaluation protocol sorts all predictions for all relations by
confidence and computes a precision-recall curve over those predictions.  We
evaluate extraction performance in aggregate (comparing predicted $Y$ values to
truth from the KB), and at the sentence level (comparing predicted $Z_i$ values
to human-annotated $Z_i$ values in the test set).

To extract PRA features, we used a dump of the Freebase graph, excluding
relations that were not relevant to our prediction task, such as the relations
under \relation{/user} and \relation{/common}, and relations dealing with
things like individual music tracks and TV episodes).  We removed any direct
edges between entity pairs in the training and test data, then ran the first
step of PRA to produce a binary feature matrix.  We then added this matrix as a
factor to the MultiR model in the two ways described in
\secref{pra-augmented-multir}.  We found that because this new factor increased
the size of the feature space, additional regularization was needed.  We added
an L2 regularization parameter to the model, which we tuned using 5-fold cross
validation on the training data.  This significantly improved the performance
of our augmented model, though a similar addition to MultiR did not improve
performance, so we did not include it in our final experiments.

We first present a comparison of aggregate extraction performance between the
ways of augmenting MultiR with PRA features.  \figref{our-comparison} shows the
results of the two different augmentations.  We also include here two simple
baselines for comparison.  ``PRA Only'' shows the result when just using PRA
features in the model (i.e., assuming that there are no sentence mentions for
any entity pair).  ``Interpolated'' is the result of a weighted
interpolation between ``PRA Only'' and MultiR, showing that joint training
between the PRA features and MultiR's dependency path features is crucial, as
the interpolated result is quite poor.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[xlabel=Recall, ylabel=Precision, no markers,width=\columnwidth]
      \addplot[red] table [y=Precision, x=Recall]{data/pra_frequent.tsv};
      \addlegendentry{PRA Per Mention}
      \addplot[blue] table [y=Precision, x=Recall]{data/pra_per_entity_frequent.tsv};
      \addlegendentry{PRA Per Entity}
      \addplot[mark=none, green] table [y=Precision, x=Recall]{data/pra_only_frequent.tsv};
      \addlegendentry{PRA Only}
      \addplot[mark=none, cyan] table [y=Precision, x=Recall]{data/interpolated.tsv};
      \addlegendentry{Interpolated}
    \end{axis}
  \end{tikzpicture}
  \caption{Aggregate extraction performance of the augmented MultiR methods
  introduced in this paper, along with two baselines.}
  \label{fig:our-comparison}
\end{figure}

Next, \figref{prior-work-comparison} shows a comparison between the best
PRA-augmented model (``PRA Per Mention'' in \figref{our-comparison}) and prior
work on aggregate extraction performance.  As can be seen from the figure, our
method substantially outperforms all prior work, including models that just
used dependency path features (MultiR and MIML-RE) and models that combined KB
embeddings with dependency path features (Weston).  It appears that the
embedding method vastly under-utilizes the knowledge base, as the same
information is available to both Weston's method and ours.  We believe this is
because much of the information contained in the knowledge base is not amenable
to low-rank representations~\cite{nickel-2014-are}, and so our method that
incorporates symbolic information from the KB can get much more leverage from
that information.  Weston's method also is a kind of interpolation between a KB
embedding model and a sentence-level embedding model, which, judging by our
interpolated result with PRA features, may explain their poor utilization of
the KB.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[xlabel=Recall, ylabel=Precision, no markers,width=\columnwidth]
      \addplot[black] table [y=Precision, x=Recall]{data/hoffmann_frequent};
      \addlegendentry{Hoffmann}
      \addplot[blue] table [y=Precision, x=Recall]{data/weston.tsv};
      \addlegendentry{Weston}
      \addplot[cyan] table [y=Precision, x=Recall]{data/miml.tsv};
      \addlegendentry{MIML-RE}
      \addplot[red] table [y=Precision, x=Recall]{data/pra_frequent.tsv};
      \addlegendentry{PRA Per Mention}
    \end{axis}
  \end{tikzpicture}
  \caption{Aggregate extraction performance of our method compared with several
  methods from prior work.}
  \label{fig:prior-work-comparison}
\end{figure}

Lastly, we present a comparison of sentential extraction performance between
MultiR and our augmented version.  We note here that the PRA features we added
are aggregate-level features, not sentence-level features.  A relation may be
true in aggregate but not expressed by a particular sentence, and so it is
possible that the PRA features could mislead the model when trying to infer
whether that relation is expressed by a specific sentence.  Because the PRA
features could be misleading at test time, we also experimented with only using
the PRA features at training time, removing them during testing.
\figref{sentential-comparison} shows the results of this comparison.
PRA-augmented MultiR performs slightly worse than MultiR, while, somewhat
surprisingly, using PRA features only at training time performs slightly
better.  We believe this improvement is because the presence of the PRA
features at training time gives the model a stronger and better supervision
signal, nudging the model to learn more from entity pairs that are strongly
associated with a relation than from those that are not, resulting in better
learned weights for the dependency features.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        xlabel=Recall,
        ylabel=Precision,
        no markers,
        width=\columnwidth,
        legend style={at={(0.03,0.2)},anchor=west},
        xmax=0.25,
        xmin=0
      ]
      \addplot[green] table [y=Precision, x=Recall]{data/senPR_withpra};
      \addlegendentry{With Test mentions}
      \addplot[red] table [y=Precision, x=Recall]{data/senPR_withouttestmention};
      \addlegendentry{Without Test Mention}
      \addplot[blue] table [y=Precision, x=Recall]{data/senPR_baseline};
      \addlegendentry{Hoffmann}
    \end{axis}
  \end{tikzpicture}
  \caption{Sentential extraction performance of our method compared with
  MultiR.}
  \label{fig:sentential-comparison}
\end{figure}

\section{Conclusion}

We have addressed the problem of incorporating features from a knowledge base
into low-level natural language processing models.  We introduced a simple
technique that computes a PRA feature matrix from a knowledge base and
incorporates it as a factor in arbitrary graphical models.  As an initial
proof-of-concept of this general technique, we showed dramatic improvements
when using this idea to augment an off-the-shelf relation extraction model,
MultiR.  We believe this result amply shows the utility of this technique, and
hope to similarly apply it to other low-level NLP models in the future.

\bibliographystyle{acl}
\bibliography{bib}

\end{document}
